{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"C:\\Users\\Gautham\\Documents\\Gautham_resume.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUTHAM BALRAJ\n",
      "+91 80729 27699 ⋄Coimbatore, TN\n",
      "gauthambalraj07@gmail.com ⋄linkedin.com/gautham-balraj ⋄github\n",
      "PROFILE\n",
      "Data science student and Generative AI enthusiast with skills in machine learning, AI, and data analytics, seeking\n",
      "an internship to apply my knowledge to real-world projects and drive innovation in the field.\n",
      "EDUCATION\n",
      "Master of Science (Integrated) , AMRITA VISHWA VIDHYAPEETHAM 2020 - Present\n",
      "Data Science\n",
      "Current CGPA: 8.04\n",
      "Secondary Education , Srinivasa Vidhyalaya March 2020\n",
      "Percentage: 82.5\n",
      "Higher Secondary Education , Srinivasa Vidhyalaya April 2018\n",
      "Percentage: 92\n",
      "SKILLS\n",
      "Technical Skills - Generative AI, Machine Learning, Deep Learning, Data Analytics, Big Data (Hadoop), NLP\n",
      "Tools & Libraries - Tensorflow, Pytorch, Langchain, Sckit-learn, PowerBI, Jupyter Lab, AWS\n",
      "Soft Skills - Communication, Problem-solving, Collaboration, Adaptability, Attention to detail\n",
      "Communication - Tamil (Native), English (Professional)\n",
      "EXPERIENCE\n",
      "Data science Intern Dec 2022 - Jan 2023\n",
      "Iamneo Coimbatore, TN\n",
      "•Spearheaded comprehensive Exploratory Data Analysis (EDA) initiatives, uncovering insights.\n",
      "•Utilized Python libraries such as Pandas, Matplotlib, and Scikit-learn to process data efficiently, automate tasks,\n",
      "and present analytical findings effectively.\n",
      "PROJECTS\n",
      "Corrective RAG (Try it here)\n",
      "–Implemented a workflow within the RAG framework that utilizes a retrieval evaluator to assess the quality\n",
      "of retrieved documents before processing.\n",
      "–This systematic workflow encompasses retrieval evaluation, corrective actions, and integration with genera-\n",
      "tive models to ensure refined and accurate text generation.\n",
      "–Utilized LangGraph, Gemini, and Groq API as key tools for improved performance.\n",
      "Auto Podcast Generator\n",
      "–Utilized voice input for topic selection and integrated multiple data sources including News API, Tavily\n",
      "Search API, and Wikipedia retriever.\n",
      "–Employed Crew AI for transcript creation and Eleven Labs for audio generation, resulting in engaging\n",
      "podcasts with dual hosts.\n",
      "–Enhanced listener experience through efficient content gathering and seamless audio production.Gmail automation.\n",
      "–Build using CrewAI, LangChain, and LangGraph to autonomously fetch and analyze emails, categorize\n",
      "them based on content, and draft responses without human intervention.\n",
      "–Integrated a voice assistant for email summarization and voice-controlled searches, enhancing user produc-\n",
      "tivity seamlessly.\n",
      "Self-Corrective Coding Agent (Ongoing):\n",
      "–Developed autonomous coding agent leveraging LangChain, Claude 3 Opus, and Mixtral.\n",
      "–Continuously enhancing capabilities to resolve dependencies, restructure code, and self-correct errors.\n",
      "–Currently in progress, with plans to incorporate additional features for further efficiency and reliability.\n",
      "NBA MVP Prediction Model.\n",
      "–Utilized ML algorithms to predict NBA MVP awards, collecting player stats and MVP data, optimizing\n",
      "model accuracy through cross-validation.\n",
      "SummarizeIt - Chrome Extension.\n",
      "–Leveraged Mixtral-8x7B model through Groq for real-time performance, enabling instant summary of web-\n",
      "page content into concise key points.\n",
      "COURSES & CERTIFICATIONS\n",
      "•Neural Networks and Deep Learning on Coursera\n",
      "•Improving Deep Neural Networks: Hyper-parameter tuning on Coursera\n",
      "•Generative AI with Large Language Models on Coursera\n",
      "•Complete Python Developer on Udemy\n",
      "•Machine Learning and Data Science Bootcamp on Udemy\n"
     ]
    }
   ],
   "source": [
    "resume_content = ''\n",
    "for page in pages:\n",
    "    resume_content += page.page_content\n",
    "print(resume_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SystemMessage = \"\"\"Act as either a recruiter. Based on the provided unstructrued content of the resume , your role to format and provide the structured content of the resume, dont make any changes to the content of the resume.\n",
    "resume content : {resume_content}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SystemMessage\n",
    "        ),\n",
    "    ])\n",
    "model = ChatOpenAI(\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\", # https://api.openai.com/v1 or https://api.groq.com/openai/v1 \n",
    "    openai_api_key= os.getenv(\"GROQ_API_KEY\"), \n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "    #model  = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"mixtral-8x7b-32768\")\n",
    "   \n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_formatted = chain.invoke({\"resume_content\":resume_content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's begin the interview. Here's my first question:\n",
      "\n",
      "Can you elaborate on your experience with Generative AI, specifically in your projects, such as Corrective RAG and Auto Podcast Generator? How did you utilize LangChain, Gemini, and Groq API in these projects, and what were some of the challenges you faced?\n",
      "\n",
      "Please respond, and I'll proceed with the next question based on your answer.\n",
      "Let's begin the interview. Here's my first question:\n",
      "\n",
      "Can you walk me through your experience with Generative AI projects, specifically how you handled data quality and model coherence in your projects, such as the Corrective RAG or Auto Podcast Generator?\n",
      "It seems like you're providing some context about Generative AI projects. That's great! However, I'd like to proceed with the interview. Here's my first question again:\n",
      "\n",
      "Can you walk me through your experience with Generative AI projects, specifically how you handled data quality and model coherence in your projects, such as the Corrective RAG or Auto Podcast Generator?\n",
      "\n",
      "Please respond to this question, and I'll proceed with the next one based on your answer.\n",
      "It seems like you're reiterating the context about Generative AI projects. Let me rephrase my previous question to help you respond more specifically.\n",
      "\n",
      "In your Corrective RAG project, how did you ensure data quality and model coherence when utilizing LangGraph, Gemini, and Groq API?\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'messages.8.content: user message cannot be an null or empty', 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend(HumanMessage(content\u001b[38;5;241m=\u001b[39muser_input))\n\u001b[1;32m---> 50\u001b[0m ai_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(ai_response)\n\u001b[0;32m     52\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend(AIMessage(content\u001b[38;5;241m=\u001b[39mai_response))\n",
      "Cell \u001b[1;32mIn[32], line 41\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(chat_history)\u001b[0m\n\u001b[0;32m     29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     30\u001b[0m [\n\u001b[0;32m     31\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     MessagesPlaceholder(variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     36\u001b[0m ])\n\u001b[0;32m     39\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\runnables\\base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    165\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    167\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    168\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    169\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    170\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    171\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    172\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    174\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    412\u001b[0m ]\n\u001b[0;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    399\u001b[0m                 m,\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    403\u001b[0m             )\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    578\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:451\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    446\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    450\u001b[0m }\n\u001b[1;32m--> 451\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\openai\\resources\\chat\\completions.py:581\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    580\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\openai\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conda\\envs\\rag_applications\\lib\\site-packages\\openai\\_base_client.py:1012\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1011\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1016\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1020\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'messages.8.content: user message cannot be an null or empty', 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "role  = \"Machine Learning Engineer\"\n",
    "level  = \"Junior\"\n",
    "number_of_questions = 4\n",
    "system_message_2 = f\"\"\"Act as an experienced Human Resources professional specializing in {role}. Your task is to conduct an interview based on the provided resume and the specified role and level.\n",
    "Resume: {resume_formatted}\n",
    "Instructions:\n",
    "\n",
    "Thoroughly review the resume and formulate {number_of_questions} relevant questions for the {level} level position.\n",
    "Prioritize questions based on the candidate's experience, projects, and skills mentioned in the resume. If needed, ask a few additional questions related to the role.\n",
    "Ask one question at a time and wait for the candidate's response before proceeding to the next question.\n",
    "If the candidate provides an unsatisfactory or incorrect answer, give a brief response and move on to the next question.\n",
    "Keep the questions concise and specific to allow for short responses from the candidate.\n",
    "If the candidate's response warrants a follow-up question, ask it before moving on to the next question.\n",
    "After asking all the questions, conclude the interview by saying, \"Thank you for your time.\"\n",
    "\n",
    "Remember to maintain a professional and objective tone throughout the interview process.\"\"\"\n",
    "def get_response(chat_history):\n",
    "    system_message = f\"\"\"Act as a seasoned Human Resources professional specializing in {role} your responsibility is to see the resume and\n",
    "                take the interview on that subject, where you will be asking only one of question for {level} level like an interviewer nothing else.\n",
    "                This is the content of the reusme {resume_formatted} refer to this and ask the question\n",
    "                Dont elaborate to much on the candidate's response just provide a short response and go for the next question\",\n",
    "                if the candidate is not able to answer the question or provided the wrong answer, just provide a short response based on it and move to next question.\n",
    "                just ask only {number_of_questions} questions one by one and based on the response  given by the candidate and go for a follow up question if needed.\n",
    "                Ask the questions in a way that the candidate can answer in a short response.\n",
    "                ask most  of the questions based on the resume and rest of the questions based on the role,\n",
    "                If there is any experience or project mentioned in the resume ask the question based on that.\n",
    "                ONLY ASK ONE QUESTION AT A TIME, when you done with asking all the questions repond with \"Thank you for your time\" \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_message_2\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "\n",
    "   \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    return chain.invoke({\n",
    "        \"messages\": chat_history\n",
    "    })\n",
    "chat_history.append(HumanMessage(content=\"lets start the interview\"))\n",
    "ai_response = get_response(chat_history)\n",
    "print(ai_response)\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    ai_response = get_response(chat_history)\n",
    "    print(ai_response)\n",
    "    chat_history.append(AIMessage(content=ai_response))\n",
    "    if len(chat_history) == number_of_questions*2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='lets start the interview'),\n",
       " HumanMessage(content='The main difference lies in their purpose: a generative model learns the joint probability distribution of the input features and labels, allowing it to generate new samples, while a discriminative model directly learns the conditional probability of the labels given the input features, focusing on classification tasks. Generative models are useful for tasks like data generation, while discriminative models excel in classification tasks, especially when labeled data is abundant.'),\n",
       " AIMessage(content=\"That's correct! Generative models, such as GANs and VAEs, learn the underlying distribution of the data and can generate new samples, whereas discriminative models, like logistic regression and SVM, focus on classification tasks and learn the conditional probability of the labels given the input features.\\n\\nHere's my next question:\\n\\nWhat is the purpose of regularization in machine learning, and how does it help in preventing overfitting?\"),\n",
       " HumanMessage(content='i guess it calculcates the mean of the samples '),\n",
       " AIMessage(content=\"I think there might be some confusion. Regularization in machine learning is not about calculating the mean of samples.\\n\\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. This penalty term discourages the model from having large weights, which can lead to overfitting. The goal of regularization is to find a balance between fitting the training data and keeping the model simple.\\n\\nLet's move on to the next question:\\n\\nWhat is the difference between bias and variance in machine learning, and how do they relate to underfitting and overfitting?\"),\n",
       " HumanMessage(content='i dont know '),\n",
       " AIMessage(content=\"That's okay! Bias and variance are important concepts in machine learning, but they can be tricky to understand.\\n\\nBias refers to the error introduced by simplifying assumptions in a model, which can lead to underfitting. Variance, on the other hand, refers to the error introduced by the model's sensitivity to the noise in the training data, which can lead to overfitting.\\n\\nDon't worry if you didn't know that - it's all part of the learning process!\\n\\nHere's my next question:\\n\\nWhat is the purpose of the activation function in a neural network, and how does it affect the output of the neuron?\"),\n",
       " HumanMessage(content='to add non leaniarity '),\n",
       " AIMessage(content=\"That's correct! The primary purpose of an activation function in a neural network is to introduce non-linearity into the model. Without an activation function, the neural network would only be able to learn linear relationships between the inputs and outputs.\\n\\nWell done!\\n\\nHere's my next question:\\n\\nWhat is the concept of gradient vanishing or exploding gradients in backpropagation, and how can it be addressed?\"),\n",
       " HumanMessage(content='i dont know '),\n",
       " AIMessage(content=\"That's okay! Gradient vanishing or exploding gradients is a common issue in deep neural networks, especially when training recurrent neural networks (RNNs).\\n\\nGradient vanishing occurs when the gradients used to update the model's weights become very small, making it difficult for the model to learn. On the other hand, exploding gradients occur when the gradients become very large, causing the model's weights to update too aggressively.\\n\\nTo address this issue, techniques such as gradient clipping, gradient normalization, and using specialized RNN architectures like LSTMs or GRUs can be used.\\n\\nNo worries if you didn't know that - it's all part of the learning process!\\n\\nThat's all the questions I had for now. Thank you for your time!\"),\n",
       " HumanMessage(content='')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyht import Client, TTSOptions, Format\n",
    "import io\n",
    "\n",
    "# Initialize PlayHT API with your credentials\n",
    "client = Client(\"g3wZk4yPnQNsFREn0MNRNadETv02\", \"c6d83fa14e4f401cb64e05e2643dddc0\")\n",
    "\n",
    "def text_to_mp3(text, filename=\"output.mp3\", voice_engine=\"PlayHT2.0-turbo\", sample_rate=44100, speed=1):\n",
    "    # Configure TTS options\n",
    "    options = TTSOptions(\n",
    "        voice=\"s3://voice-cloning-zero-shot/9fc626dc-f6df-4f47-a112-39461e8066aa/oliviaadvertisingsaad/manifest.json\",\n",
    "        sample_rate=sample_rate,\n",
    "        format=Format.FORMAT_MP3,\n",
    "        speed=speed\n",
    "    )\n",
    "\n",
    "    # Generate audio stream\n",
    "    audio_stream = io.BytesIO()\n",
    "    for chunk in client.tts(text=text, voice_engine=voice_engine, options=options):\n",
    "        audio_stream.write(chunk)\n",
    "\n",
    "    # Write audio stream to file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(audio_stream.getvalue())\n",
    "\n",
    "# Example usage\n",
    "text = \"Hey, this is Jennifer from Play.\"\n",
    "text_to_mp3(text, \"output.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi again. It seems like we got disconnected for a moment. Let's get back on track. You were about to share an example of a project where you've used one of the tools you've listed. Could you please share that with me?\n"
     ]
    }
   ],
   "source": [
    "# `pip3 install assemblyai` (macOS)\n",
    "# `pip install assemblyai` (Windows)\n",
    "\n",
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = \"d0d54f4f51284c419b0dec49548b5e3a\"\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "transcript = transcriber.transcribe(\"E:\\code_files\\Banao_Intern\\AI_Interview\\output.mp3\")\n",
    "# transcript = transcriber.transcribe(\"./my-local-audio-file.wav\")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 4.80s]  Welcome to our Augmented Interview. Please start by telling us a bit about yourself.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\"tiny\")\n",
    "\n",
    "segments, info = model.transcribe(\"E:\\code_files\\Banao_Intern\\AI_Interview\\output.mp3\")\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Emergency. Gas leak detected on your premises evacuate immediately.'}\n"
     ]
    }
   ],
   "source": [
    "# pip install requests\n",
    "import requests\n",
    "\n",
    "url = \"https://api.lemonfox.ai/v1/audio/transcriptions\"\n",
    "headers = {\n",
    "  \"Authorization\": \"buu3zOdoZWbu0K3WHUDQbTGOcteeKaEP\"\n",
    "}\n",
    "data = {\n",
    "  \"file\": \"E:\\code_files\\Banao_Intern\\AI_Interview\\\\alert_gas_leak.mp3\",\n",
    "  \"language\": \"english\",\n",
    "  \"response_format\": \"json\"\n",
    "}\n",
    "\n",
    "# To upload a local file add the files parameter:\n",
    "files = {\"file\": open(\"E:\\code_files\\Banao_Intern\\AI_Interview\\\\alert_gas_leak.mp3\", \"rb\")}\n",
    "response = requests.post(url, headers=headers, files=files, data=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"buu3zOdoZWbu0K3WHUDQbTGOcteeKaEP\",\n",
    "  base_url=\"https://api.lemonfox.ai/v1\",\n",
    ")\n",
    "\n",
    "audio_file = open(r\"E:\\code_files\\Banao_Intern\\AI_Interview\\output.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file,\n",
    "  language=\"en\",\n",
    "  response_format=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to our augmented interview. Please start by telling us a bit about yourself.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import os \n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "import streamlit as st\n",
    "from streamlit_modal import Modal\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from pyht import Client, TTSOptions, Format\n",
    "import io\n",
    "from utils import text_to_mp3, autoplay_audio\n",
    "from audio_test import record_text,transcribe_audio\n",
    "import time\n",
    "from streamlit.components.v1 import html\n",
    "from streamlit_js_eval import streamlit_js_eval\n",
    "from colour_print import print_red, print_green, print_yellow, print_blue, print_magenta, print_cyan, print_white\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "## class for structured output (pydantic)\n",
    "\n",
    "class percentage(BaseModel):\n",
    "    score: int = Field(description=\"The percentage score for the candidate's performance in the interview.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = \"\"\"\"\n",
    "Interviewer: Hi, thank you for joining us today. Could you start by telling me a bit about yourself and your professional background?\n",
    "\n",
    "Candidate: Certainly. My name is Alex Johnson,\n",
    "\n",
    "Interviewer: Great. Can you describe a challenging project you worked on recently and how you handled it?\n",
    "\n",
    "Candidate: can we skip thiss \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def evaluate_percentage(chat_history,role,level):\n",
    "    evaluation_prompt = \"\"\"\n",
    "    As an experienced HR professional, review the following interview conversation history to evaluate the candidate's performance for the role of {role} at the {level} difficulty. The evaluation should consider various aspects of the interview process and be strict in assigning a percentage score based on the provided conversation history.\n",
    "    Conversation History:\n",
    "    {conversation_history}\n",
    "\n",
    "    \n",
    "    \n",
    "    Please provide a percentage score (don't add any preamble or explanation):\n",
    "    \"\"\"\n",
    "\n",
    "    chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "\n",
    "    structued_llm = chat.with_structured_output(percentage)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You're an experienced HR professional evaluating a candidate's interview performance.\"),\n",
    "            (\"human\", evaluation_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | structued_llm \n",
    "    \n",
    "\n",
    "\n",
    "    return chain.invoke({\"conversation_history\": chat_history, \"role\": role, \"level\": level})\n",
    "result = evaluate_percentage(chat,\"Machine Learning Engineer\",\"Junior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentage(score=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "**Summary of Key Points Discussed:**\n",
      "\n",
      "Unfortunately, the conversation history is quite limited, and the candidate failed to provide any meaningful information about themselves or their professional background. The candidate was asked to describe a challenging project they worked on recently and how they handled it, but instead of answering the question, they requested to skip it.\n",
      "\n",
      "**Performance Rating:**\n",
      "\n",
      "Based on the conversation history, I would assign a performance rating of 10% to the candidate. The candidate failed to provide any relevant information about themselves or their experience, and they demonstrated a lack of willingness to answer a critical question about their problem-solving skills.\n",
      "\n",
      "**Fit for the Role:**\n",
      "\n",
      "Based solely on the conversation, I would assess that the candidate is not a good fit for the role. The candidate's inability to provide basic information about themselves and their experience raises concerns about their communication skills, motivation, and willingness to engage in a meaningful conversation. Additionally, their request to skip a critical question about their problem-solving skills suggests that they may not be prepared to discuss their experiences and skills in a way that is relevant to the role.\n",
      "\n",
      "**Evaluation of Aspects:**\n",
      "\n",
      "* **Clarity of Communication:** 20% - The candidate failed to provide clear and concise information about themselves and their experience.\n",
      "* **Relevance of Answers:** 0% - The candidate did not provide any relevant answers to the questions asked.\n",
      "* **Depth of Knowledge:** N/A - The candidate did not demonstrate any knowledge or understanding of the subject matter.\n",
      "* **Overall Impression:** 10% - The candidate's overall performance was poor, and they failed to demonstrate any enthusiasm or motivation for the role.\n",
      "\n",
      "Overall, the candidate's performance was disappointing, and they failed to demonstrate any of the skills or qualities that would make them a strong fit for the role.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class InterviewEvaluation(BaseModel):\n",
    "    percentage: int = Field(..., description=\"The strict performance rating percentage based on the interview conversation history.\")\n",
    "    review: str = Field(..., description=\"A comprehensive evaluation of the candidate's performance based on the interview conversation history.\")\n",
    "\n",
    "def evaluate_interview(chat_history, role, level):\n",
    "    # Evaluation prompt for detailed review\n",
    "    review_prompt = \"\"\"\n",
    "    As an experienced HR professional, review the following conversation history of an interview \n",
    "    and provide a comprehensive evaluation of the candidate's performance. Your evaluation should include:\n",
    "\n",
    "    1. Summary of Key Points Discussed: Highlight the main topics and points covered during the interview.\n",
    "    2. Performance Rating: Assign a performance rating as a percentage based on the candidate's responses.\n",
    "    3. Fit for the Role: Based solely on the conversation (and not the resume), assess whether the candidate \n",
    "       seems to be a good fit for the role.\n",
    "\n",
    "    When evaluating, consider the following aspects:\n",
    "    - Clarity of Communication: How clearly did the candidate express their thoughts and ideas?\n",
    "    - Relevance of Answers: Were the candidate’s answers pertinent to the questions asked and the role applied for?\n",
    "    - Depth of Knowledge: Did the candidate demonstrate a thorough understanding of the subject matter?\n",
    "    - Overall Impression: What was your overall impression of the candidate's suitability for the role?\n",
    "\n",
    "    Conversation History:\n",
    "    {chat_history}\n",
    "\n",
    "    Please provide a detailed summary, a performance rating percentage, and an assessment of the candidate's fit for the role based solely on the conversation.\n",
    "    Important Note: Ensure each user response in the conversation history fully answers the question asked without beating around the bush.\n",
    "    and be very strict in rating the performance of the candidate and fitness for the role.\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluation prompt for percentage score\n",
    "    percentage_prompt = \"\"\"\n",
    "    As an experienced HR professional, review the following interview conversation history to evaluate the candidate's performance for the role of {role} at the {level} difficulty. The evaluation should consider various aspects of the interview process and be strict in assigning a percentage score based on the provided conversation history.\n",
    "\n",
    "    Conversation History:\n",
    "    {chat_history}\n",
    "\n",
    "\n",
    "    Note: dont provide any explanation or preamble, just provide the percentage score based on the conversation history.\n",
    "    Please provide a percentage score (don't add any preamble or explanation):\n",
    "    \"\"\"\n",
    "\n",
    "    # Model for detailed review\n",
    "    review_model = ChatOpenAI(\n",
    "        openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "        openai_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        model_name=\"llama3-70b-8192\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Model for percentage score\n",
    "    percentage_model = ChatGroq(\n",
    "        temperature=0,\n",
    "        model=\"llama3-70b-8192\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "\n",
    "    # Create prompt templates\n",
    "    prompt_template_review = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You're an experienced HR professional evaluating a candidate's interview performance.\"),\n",
    "            (\"human\", review_prompt),\n",
    "        ]\n",
    "    )\n",
    "    prompt_template_precentage = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You're an experienced HR professional evaluating a candidate's interview performance.\"),\n",
    "            (\"human\", percentage_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create chains\n",
    "    review_chain = prompt_template_review | review_model | StrOutputParser()\n",
    "    percentage_chain =  prompt_template_precentage | percentage_model.with_structured_output(percentage)    \n",
    "\n",
    "    # Invoke the chains\n",
    "    review_result = review_chain.invoke({\"chat_history\": chat_history})\n",
    "    percentage_result = percentage_chain.invoke({\"chat_history\": chat_history, \"role\": role, \"level\": level})\n",
    "\n",
    "    # Return results in a Pydantic object\n",
    "    return InterviewEvaluation(\n",
    "        percentage=percentage_result.score,\n",
    "        review=review_result\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "\n",
    "role = \"Machine Learning Engineer\"\n",
    "level = \"Junior\"\n",
    "result = evaluate_interview(chat, role, level)\n",
    "print(result.percentage, result.review,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x00000212A94023E0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000212A90F1E40> model_name='gpt-3.5-turbo-0125' temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "['Machine Learning Algorithms', 'Deep Learning Architectures', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-gautham-ppZK3HhAOqSXO2jIOkqnT3BlbkFJXobfeGpJZ4KAQfrbsur3\"\n",
    "\n",
    "# Define InterviewAreas model\n",
    "class InterviewAreas(BaseModel):\n",
    "    areas: List[str] = Field(\n",
    "        description=\"List of 5 main areas to test in the interview\",\n",
    "    )\n",
    "\n",
    "# Initialize LLMs\n",
    "gpt = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "groq = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "def get_interview_areas(role: str, years_of_experience: str, llm = groq) -> InterviewAreas:\n",
    "    system_msg = f\"\"\"\n",
    "    For this given 'Role: {role} | Experience: {years_of_experience} years', provide 5 very important core technical concepts to test in an interview.\n",
    "    Output the areas in JSON format: {{\"areas\": [\"area1\", \"area2\", \"area3\", \"area4\", \"area5\"]}}\n",
    "    just provide the output dont add any preamble or explanation\n",
    "    \"\"\"\n",
    "    llm_with_structure = llm.with_structured_output(InterviewAreas)\n",
    "    ai_msg = llm_with_structure.invoke(system_msg)\n",
    "    print(llm)\n",
    "    return ai_msg.areas\n",
    "\n",
    "# Example usage\n",
    "role = \"AI Engineer\"\n",
    "years_of_experience = \"3\"\n",
    "llm = gpt\n",
    "\n",
    "interview_areas = get_interview_areas(role, years_of_experience,llm=gpt)\n",
    "print(interview_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat  = \"\"\"\n",
    "Interviewer: Welcome, thanks for joining us today. Can you start by telling me a bit about your experience with machine learning?\n",
    "\n",
    "Candidate: Absolutely. I have been working in machine learning for the past five years. I started with supervised learning techniques, then moved on to unsupervised learning, and now I’m heavily involved in deep learning projects.\n",
    "\n",
    "Interviewer: That's great to hear. Can you give an example of a project where you applied deep learning?\n",
    "\n",
    "Candidate: Sure, one of the recent projects I worked on was developing a convolutional neural network for image classification. We used TensorFlow and Keras to build the model, and it significantly improved the accuracy compared to our previous methods.\n",
    "\n",
    "Interviewer: Interesting. Could you explain the preprocessing steps you followed for the image data?\n",
    "\n",
    "Candidate: Of course. We performed several preprocessing steps including image resizing, normalization, and augmentation techniques like rotation and flipping to increase the diversity of our training data.\n",
    "\n",
    "Interviewer: That sounds comprehensive. How did you handle model evaluation and validation?\n",
    "\n",
    "Candidate: We used a combination of techniques such as cross-validation, confusion matrix analysis, and ROC curves. Additionally, we set aside a validation set from our training data to fine-tune the model’s hyperparameters.\n",
    "\n",
    "Interviewer: Can you describe a challenge you faced during this project and how you overcame it?\n",
    "\n",
    "Candidate: One major challenge was dealing with overfitting. Initially, our model performed exceptionally well on the training data but poorly on the validation set. We tackled this by implementing dropout layers and using early stopping during training.\n",
    "\n",
    "Interviewer: Very insightful. Now, can you discuss your experience with deploying machine learning models in a production environment?\n",
    "\n",
    "Candidate: Sure. In my current role, I’ve deployed several models using Docker and Kubernetes. We often use CI/CD pipelines to automate the deployment process, ensuring that our models can be easily updated and scaled.\n",
    "\n",
    "Interviewer: Great. Lastly, could you tell me about a time you had to collaborate with a team to complete a machine learning project?\n",
    "\n",
    "Candidate: Collaboration is key in our projects. For the image classification project, I worked closely with data engineers to manage the data pipeline and with software developers to integrate the model into our application. Regular meetings and clear communication were crucial to our success.\n",
    "\n",
    "Interviewer: Thank you for sharing your experiences. That’s all the questions I have for now. Do you have any questions for me?\n",
    "\n",
    "Candidate: Not at the moment, but thank you for the opportunity to discuss my experience.\n",
    "\n",
    "Interviewer: Thank you for your time.\n",
    "\"\"\"\n",
    "role = \"Machine Learning Engineer\"\n",
    "years_of_experience = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ExperiencewithMachineLearning': 90,\n",
       " 'ProjectExamples': 85,\n",
       " 'DataPreprocessingTechniques': 80,\n",
       " 'ModelEvaluationandValidation': 85,\n",
       " 'Problem-Solving': 85,\n",
       " 'DeploymentofMLModels': 90,\n",
       " 'CollaborationandTeamwork': 85}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Dict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json \n",
    "\n",
    "\n",
    "class AreaPercentageModel(BaseModel):\n",
    "    areas: Dict[str, int] = Field(description=\"Areas with respective percentages\")\n",
    "    @classmethod\n",
    "    def from_json_string(cls, json_string: str):\n",
    "        # Remove newline characters and extra spaces\n",
    "        clean_string = json_string.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "        # Parse the cleaned string into a dictionary\n",
    "        data = json.loads(clean_string)\n",
    "        return cls(areas=data)\n",
    "def skills_based(chat_history, role, level, areas):\n",
    "    review_prompt = \"\"\"\\\n",
    "    As an experienced HR professional, review the following interview conversation history to evaluate the candidate's performance for the role of {role} at the {level} difficulty. The evaluation should be strictly based on the provided conversation history.\n",
    "\n",
    "    Please provide a percentage for each area in this list: {areas} based on the conversation history. The output should be in JSON format: {{\"area1\": percentage, \"area2\": percentage, ...}}. Just provide the output, don't add any preamble or explanation. Don't provide 0 percentage for any area, provide the percentage based on the conversation history.\n",
    "    \n",
    "    Conversation History:\n",
    "    {conversation_history}\n",
    "    \"\"\"\n",
    "\n",
    "    structued_llm = gpt.with_structured_output(AreaPercentageModel)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You're an experienced HR professional evaluating a candidate's interview performance.\"),\n",
    "            (\"human\", review_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | gpt \n",
    "    result = chain.invoke({\"conversation_history\": chat_history, \"role\": role, \"level\": level, \"areas\": areas})\n",
    "    result = AreaPercentageModel.from_json_string(result.content)\n",
    "    return result.areas\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "skills_based(chat,role,years_of_experience,interview_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
